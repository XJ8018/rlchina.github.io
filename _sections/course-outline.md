--- 
title: 课程大纲 
icon: fa-th 
order: 4 
--- 
<p style="text-align:justify; text-justify:inter-ideograph;color: black">
1、卢宗青: Value-based Reinforcement Learning <br />
- Introduction to Reinforcement Learning <br />
    <p style="text-indent:2em;">- About RL <br /></p>
    <p style="text-indent:2em;">- RL problem <br /></p>
    <p style="text-indent:2em;">- Inside an RL agent <br /></p>
    <p style="text-indent:2em;">- Markov Decision Processes <br /></p>
- Value-based Methods <br />
    <p style="text-indent:2em;">- Dynamic Programming <br /></p>
    <p style="text-indent:2em;">- Monte Carlo <br /></p>
    <p style="text-indent:2em;">- TD Learning <br /></p>
    <p style="text-indent:2em;">- Off-policy Learning <br />
    <p style="text-indent:2em;">- DQN and its variants <br /></p>
<br />
2、汪军: Policy-based RL and RL Theory <br />
- Policy based approaches <br />
    <p style="text-indent:2em;">- Policy gradient theorem <br /></p>
    <p style="text-indent:2em;">- REINFORCE algorithm <br /></p>
    <p style="text-indent:2em;">- Natural policy gradient <br /></p>
- PAC Learning theory <br />
    <p style="text-indent:2em;">- Definition and concepts <br /></p>
    <p style="text-indent:2em;">- Concentration inequalities <br /></p>
    <p style="text-indent:2em;">- Uniform convergence <br /></p>
- RL theory <br />
    <p style="text-indent:2em;">- Approximate Dynamic Programming <br /></p>
    <p style="text-indent:2em;">- Approximate Value Iteractions <br /></p>
    <p style="text-indent:2em;">- Approximate Policy Iterations <br /></p>
    <p style="text-indent:2em;">- Theoretical bounds and sample complexity analysis <br /></p>
<br />
3、Haitham: Optimisation in Learning <br />
- Motivation, Functions & Solution Types <br />
    <p style="text-indent:2em;">- Applications of optimisation in Machine Learning <br /></p>
    <p style="text-indent:2em;">- Convex vs Non-Convex Optimisation Techniques <br /></p>
    <p style="text-indent:2em;">- Non-Convex Optimisation Solution Types <br /></p>
- Brief Survey of Optimisation Methods – Merits & Demerits <br />
    <p style="text-indent:2em;">- Zero-Order Techniques <br /></p>
    <p style="text-indent:2em;">- First-Order Techniques <br /></p>
    <p style="text-indent:2em;">- Second-Order Techniques <br /></p>
- ADAM: An Adaptive Solver <br />
    <p style="text-indent:2em;">- Brief History of ADAM <br /></p>
    <p style="text-indent:2em;">- ADAM’s Description <br /></p>
- ADAM’s Convergence Proof: <br />
    <p style="text-indent:2em;">- Proof Strategy <br /></p>
    <p style="text-indent:2em;">- Assumptions <br /></p>
    <p style="text-indent:2em;">- Loss Function Difference Bound and Stationary Point Convergence <br /></p>
<br />
4、张伟楠: Model-based Reinforcement Learning <br />
- Model-based RL concepts <br />
    <p style="text-indent:2em;"><p style="text-indent:2em;">- Blackbox & whitebox models <br /></p>
- Classic MBRL <br />
    <p style="text-indent:2em;"><p style="text-indent:2em;">- Q-planning <br /></p>
    <p style="text-indent:2em;"><p style="text-indent:2em;">- Dyna-Q <br /></p>
- Blackbox MBRL <br />
    <p style="text-indent:2em;"><p style="text-indent:2em;">- Model Predictive Control <br /></p>
    <p style="text-indent:2em;"><p style="text-indent:2em;">- Probabilistic Ensemble & Trajectory Sampling <br /></p>
    <p style="text-indent:2em;"><p style="text-indent:2em;">- Stochastic Lower Bound Optimization <br /></p>
    <p style="text-indent:2em;"><p style="text-indent:2em;">- Model-based Policy Optimization <br /></p>
    <p style="text-indent:2em;"><p style="text-indent:2em;">- Bidirectional Model based Policy Optimization <br /></p>
- Whitebox MBRL <br />
    <p style="text-indent:2em;"><p style="text-indent:2em;">- Stochastic Value Gradient <br />
    <p style="text-indent:2em;"><p style="text-indent:2em;">- Model Augmented Actor Critic <br />
<br />
5、朱占星: Control as Inference  <br />
- Basics of probabilistic graphical models (D-separation, variational inference.) <br />
- Connection between reinforcement learning and probabilistic inference.  <br />
- Soft Q-learning  <br />
- Entropy-regularized policy gradient <br />
<br />
6、俞扬: Imitation Learning  <br />
- Preliminary <br />
- Supervised Learning & Behavior Cloning <br />
- Generative Adversarial Learning & GAIL <br />
- Advanced Topics <br />
- From Imitating Policies to Imitating Environments <br />
<br />
7、郝建业: Hierarchical Reinforcement Learning <br />
- Long-episode and sparse-reward RL tasks <br />
- Hierarchical RL Concepts <br />
- Option Framework <br />
- Feudal Network & Variants <br />
- Goal-oriented RL <br />
<br />
8、张海峰: Game Theory Basics  <br />
- Motivation and Normal-form Game <br />
- NE Concepts and Existence Proof <br />
- Repeated Game and Tit-for-tat <br />
- Extensive-form Game and Subgame Perfect Equilibrium  <br />
- Other Solution Concepts <br />
<br />
9、安波: Multi-agent Systems  <br />
- History and Current Status <br />
- Multi-agent Coordination and Negotiation <br />
- Multi-agent Planning and Teamwork <br />
- Distributed Constraint Optimization <br />
- Multi-agent Organizational Design <br />
- Game Theory and Mechanism Design <br />
- Game Theory for Security <br />
<br />
10、张崇洁: Deep Multi-agent Learning <br />
- Multi-Agent MDP and Dec-POMDP <br />
- Centralized Training and Decentralized Execution and Individual-Global Maximization Principle <br />
- Value-Based Deep MARL Methods <br />
- Multi-Agent Policy Gradient Methods <br />
- Theoretical Analysis  <br />
<br />
11、杨耀东: Advances in Multi-agent Learning <br />
- Extensive-form games and its basics <br />
- Ficiticious play <br />
- Generalised weakened Ficiticious play <br />
- Policy Space Response Oracle (PSRO) <br />
- Alpha-Rank <br />
- PSRO-Nash <br />
- PSRO-Rechtified Nash <br />
- PSRO-AlphaRank <br />
<br />
12、徐任远: Mean-field Games and Controls <br />
- Mean-field game models  <br />
    <p style="text-indent:2em;">- Examples  <br /></p>>
    <p style="text-indent:2em;">- Approximation of non-cooperative games in the large population regime  <br /></p>
    <p style="text-indent:2em;">- Equilibrium concept  <br /></p>
- Existence and uniqueness of the equilibrium  <br />
- Q-learning based algorithm  <br />
    <p style="text-indent:2em;">- Stabilizing and smoothing techniques  <br /></p>
    <p style="text-indent:2em;">- Convergence and sample complexity analysis  <br /></p>
- General algorithms for learning mean-fifield games  <br />
    <p style="text-indent:2em;">- Value-based algorithms (light touch)  <br /></p>
    <p style="text-indent:2em;">- Policy-based algorithms (light touch) <br /></p>
- Mean-field control models   <br />
    <p style="text-indent:2em;">- Examples  <br /></p>
    <p style="text-indent:2em;">- Approximation of cooperative games in the large population regime  <br /></p>
    <p style="text-indent:2em;">- Social optimality condition  <br /></p>
- Dynamic programming principle on the probability measure space  <br />
- Q-learning based algorithm  <br />
    <p style="text-indent:2em;">- Kernel regression  <br /></p>
    <p style="text-indent:2em;">- Approximated Bellman update  <br /></p>
    <p style="text-indent:2em;">- Convergence and sample complexity analysis <br /></p>
</p>
